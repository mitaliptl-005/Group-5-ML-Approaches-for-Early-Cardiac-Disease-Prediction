{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IuULr04XiRse",
        "outputId": "ea054e61-0790-4979-b96a-4c1da89ac6a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, random_split, DataLoader, Subset\n",
        "from scipy.ndimage import rotate\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "PyYbRgL5ikMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHawJ6oR7Z-Z"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# 1Ô∏è‚É£ Set paths for already uploaded files\n",
        "# -----------------------------\n",
        "train_pkl = '/content/drive/MyDrive/Code/train_classification.pkl'\n",
        "test_pkl  = '/content/drive/MyDrive/Code/test_classification.pkl'\n",
        "\n",
        "import os\n",
        "print(\"Train exists:\", os.path.exists(train_pkl))\n",
        "print(\"Test exists:\", os.path.exists(test_pkl))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ACDCClassificationDataset(Dataset):\n",
        "    def __init__(self, pkl_path, use_both_phases=True, target_shape=(128,128,8), mean=None, std=None):\n",
        "        with open(pkl_path, 'rb') as f:\n",
        "            self.data = pickle.load(f)\n",
        "        self.ids = list(self.data.keys())\n",
        "        self.use_both = use_both_phases\n",
        "        self.target_shape = target_shape\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "        label_map = {'NOR':0,'MINF':1,'DCM':2,'HCM':3,'RV':4}\n",
        "        self.labels = [label_map.get(self.data[i]['Label'], -1) for i in self.ids]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "\n",
        "    def _resize_volume(self, vol):\n",
        "        # vol: [H, W, D]\n",
        "        vol_t = torch.tensor(vol[np.newaxis, np.newaxis, ...], dtype=torch.float32)  # [1,1,H,W,D]\n",
        "        resized = torch.nn.functional.interpolate(vol_t, size=self.target_shape, mode='trilinear', align_corners=False)\n",
        "        return resized.squeeze(0).squeeze(0).numpy()  # remove only first two singleton dims\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        pid = self.ids[idx]\n",
        "        sample = self.data[pid]\n",
        "\n",
        "        # Resize ED and ES volumes\n",
        "        ed = self._resize_volume(sample['ED'].astype(np.float32))  # [128,128,8]\n",
        "        es = self._resize_volume(sample['ES'].astype(np.float32))  # [128,128,8]\n",
        "\n",
        "        # Normalization\n",
        "        if self.mean is not None and self.std is not None:\n",
        "            ed = (ed - self.mean) / (self.std + 1e-8)\n",
        "            es = (es - self.mean) / (self.std + 1e-8)\n",
        "\n",
        "        # Stack phases\n",
        "        vol = np.stack([ed, es], axis=0) if self.use_both else ed[np.newaxis, ...]  # [2,H,W,D] or [1,H,W,D]\n",
        "\n",
        "        # Reorder axes to C √ó D √ó H √ó W for PyTorch 3D CNN\n",
        "        vol = np.transpose(vol, (0, 3, 1, 2))  # [C, D, H, W]\n",
        "\n",
        "        label = self.labels[idx]\n",
        "        return torch.tensor(vol, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n"
      ],
      "metadata": {
        "id": "ynstJZ3h9b9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Compute mean and std (training set only)\n",
        "# -----------------------------\n",
        "full_dataset_tmp = ACDCClassificationDataset(train_pkl, use_both_phases=True, target_shape=(128,128,8))\n",
        "all_vols = []\n",
        "for pid in full_dataset_tmp.ids:\n",
        "    sample = full_dataset_tmp.data[pid]\n",
        "    all_vols.append(sample['ED'].astype(np.float32).flatten())\n",
        "    all_vols.append(sample['ES'].astype(np.float32).flatten())\n",
        "all_vols = np.concatenate(all_vols)\n",
        "mean = all_vols.mean()\n",
        "std = all_vols.std()\n",
        "print(f\"Training mean: {mean:.4f}, std: {std:.4f}\")"
      ],
      "metadata": {
        "id": "Ypi_d70Ni9aF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Split dataset (train/val/test)\n",
        "# -----------------------------\n",
        "target_shape = (128,128,8)\n",
        "\n",
        "# Full dataset with normalization for training\n",
        "full_dataset = ACDCClassificationDataset(\n",
        "    train_pkl,\n",
        "    use_both_phases=True,\n",
        "    target_shape=target_shape,\n",
        "    mean=mean,\n",
        "    std=std\n",
        ")\n",
        "\n",
        "# Split train/validation\n",
        "train_size = int(0.8 * len(full_dataset))   # 80% train\n",
        "val_size = len(full_dataset) - train_size  # 20% validation\n",
        "train_dataset, val_dataset = random_split(\n",
        "    full_dataset,\n",
        "    [train_size, val_size],\n",
        "    generator=torch.Generator().manual_seed(42)\n",
        ")\n",
        "\n",
        "# DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=2)\n",
        "\n",
        "# Test dataset (no augmentation)\n",
        "test_dataset = ACDCClassificationDataset(\n",
        "    test_pkl,\n",
        "    use_both_phases=True,\n",
        "    target_shape=target_shape,\n",
        "    mean=mean,\n",
        "    std=std\n",
        ")\n",
        "test_loader  = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=2)\n",
        "\n",
        "print(f\"Train samples: {len(train_dataset)}, Val samples: {len(val_dataset)}, Test samples: {len(test_dataset)}\")\n"
      ],
      "metadata": {
        "id": "uKm5h6rP9niq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ==========================\n",
        "# 3D CNN Model\n",
        "# ==========================\n",
        "class model_3DCNN(nn.Module):\n",
        "    def __init__(self, num_classes=5, in_channels=2):\n",
        "        super(model_3DCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv3d(in_channels, 32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm3d(32)\n",
        "        self.pool1 = nn.MaxPool3d(2)\n",
        "\n",
        "        self.conv2 = nn.Conv3d(32, 64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm3d(64)\n",
        "        self.pool2 = nn.MaxPool3d(2)\n",
        "\n",
        "        self.conv3 = nn.Conv3d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm3d(128)\n",
        "        self.pool3 = nn.MaxPool3d(2)\n",
        "\n",
        "        self.conv4 = nn.Conv3d(128, 256, kernel_size=3, padding=1)\n",
        "        self.bn4 = nn.BatchNorm3d(256)\n",
        "        self.pool4 = nn.AdaptiveAvgPool3d(1)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = self.pool3(x)\n",
        "\n",
        "        x = F.relu(self.bn4(self.conv4(x)))\n",
        "        x = self.pool4(x)\n",
        "\n",
        "        x = x.flatten(start_dim=1)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# ==========================\n",
        "# Device, Model, Loss, Optimizer\n",
        "# ==========================\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model_3DCNN(num_classes=5, in_channels=2).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=3e-, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)\n",
        "\n",
        "# ==========================\n",
        "# Metrics storage\n",
        "# ==========================\n",
        "history = {\"train_loss\": [], \"val_loss\": [], \"train_acc\": [], \"val_acc\": []}\n",
        "num_epochs = 100\n",
        "\n",
        "# ==========================\n",
        "# Training Loop\n",
        "# =========================\n",
        "metrics_csv = \"/content/training_history.csv\"\n",
        "\n",
        "# ==========================\n",
        "# Training Loop with CSV logging\n",
        "# ==========================\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    start_time = time.time()\n",
        "\n",
        "    # --- Training ---\n",
        "    model.train()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "\n",
        "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs}\", leave=False):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * labels.size(0)\n",
        "        _, predicted = outputs.max(1)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    train_loss = running_loss / total\n",
        "    train_acc = correct / total\n",
        "\n",
        "    # --- Validation ---\n",
        "    model.eval()\n",
        "    val_loss, val_correct, val_total = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item() * labels.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            val_correct += predicted.eq(labels).sum().item()\n",
        "            val_total += labels.size(0)\n",
        "\n",
        "    val_loss /= val_total\n",
        "    val_acc = val_correct / val_total\n",
        "\n",
        "    # Scheduler step\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    # Store metrics in dictionary\n",
        "    history[\"train_loss\"].append(train_loss)\n",
        "    history[\"val_loss\"].append(val_loss)\n",
        "    history[\"train_acc\"].append(train_acc)\n",
        "    history[\"val_acc\"].append(val_acc)\n",
        "\n",
        "    # Save metrics to CSV\n",
        "    df = pd.DataFrame({\n",
        "        \"epoch\": list(range(1, epoch + 1)),\n",
        "        \"train_loss\": history[\"train_loss\"],\n",
        "        \"val_loss\": history[\"val_loss\"],\n",
        "        \"train_acc\": history[\"train_acc\"],\n",
        "        \"val_acc\": history[\"val_acc\"]\n",
        "    })\n",
        "    df.to_csv(metrics_csv, index=False)\n",
        "\n",
        "    elapsed = time.time() - start_time\n",
        "    print(f\"Epoch {epoch}/{num_epochs} | \"\n",
        "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
        "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f} | \"\n",
        "          f\"Time: {elapsed:.1f}s\")\n",
        "\n",
        "print(f\"\\nTraining history saved to {metrics_csv}\")"
      ],
      "metadata": {
        "id": "0yABEiSMT2ON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================\n",
        "# üîπ Plot training history with accuracy limit\n",
        "# ==========================\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Loss plot\n",
        "plt.figure(figsize=(12,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(history['train_loss'], label='Train Loss', color='blue')\n",
        "plt.plot(history['val_loss'], label='Validation Loss', color='red')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training vs Validation Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Accuracy plot\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(history['train_acc'], label='Train Accuracy', color='blue')\n",
        "plt.plot(history['val_acc'], label='Validation Accuracy', color='red')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training vs Validation Accuracy')\n",
        "plt.ylim(0, 1)  # ‚úÖ Set accuracy limits from 0 to 1\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "q52JVSloZrRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "# Make sure the model is in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:  # your test dataloader\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# ==========================\n",
        "# Test Accuracy\n",
        "# ==========================\n",
        "correct = sum([p == t for p, t in zip(all_preds, all_labels)])\n",
        "total = len(all_labels)\n",
        "test_acc = correct / total\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# ==========================\n",
        "# Confusion Matrix\n",
        "# ==========================\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix - Test Set\")\n",
        "plt.show()\n",
        "\n",
        "# ==========================\n",
        "# Classification Report\n",
        "# ==========================\n",
        "report = classification_report(all_labels, all_preds, target_names=['Class 0','Class 1','Class 2','Class 3','Class 4'])\n",
        "print(\"Classification Report:\\n\", report)\n"
      ],
      "metadata": {
        "id": "_va3fOqJT_R7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Class labels\n",
        "labels = [\"Normal\", \"DCM\", \"HCM\", \"MINF\", \"ARV\"]\n",
        "\n",
        "# Metrics from classification report\n",
        "precision = [0.759, 0.707, 0.709, 0.733, 0.814]\n",
        "recall = [0.820, 0.750, 0.730, 0.740, 0.790]\n",
        "f1 = [0.788, 0.728, 0.719, 0.736, 0.802]\n",
        "\n",
        "# Set bar positions\n",
        "x = np.arange(len(labels))\n",
        "width = 0.25\n",
        "\n",
        "# Create plot\n",
        "plt.figure(figsize=(8, 5))\n",
        "bars1 = plt.bar(x - width, precision, width, label='Precision')\n",
        "bars2 = plt.bar(x, recall, width, label='Recall')\n",
        "bars3 = plt.bar(x + width, f1, width, label='F1-score')\n",
        "\n",
        "\n",
        "# Labels and formatting\n",
        "plt.xlabel(\"Group\", fontsize=11)\n",
        "plt.ylabel(\"Score\", fontsize=11)\n",
        "plt.title(\"Classification Metrics per Class-using Image\", fontsize=13)\n",
        "plt.xticks(x, labels)\n",
        "plt.ylim(0, 1)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mdlQNOO38Zpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# ==========================\n",
        "# ROC Curve Evaluation\n",
        "# ==========================\n",
        "model.eval()\n",
        "all_labels = []\n",
        "all_probs = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in val_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        outputs = model(inputs)\n",
        "        probs = F.softmax(outputs, dim=1).cpu().numpy()\n",
        "        all_probs.append(probs)\n",
        "        all_labels.append(labels.cpu().numpy())\n",
        "\n",
        "# Concatenate predictions and labels\n",
        "all_labels = np.concatenate(all_labels)\n",
        "all_probs = np.concatenate(all_probs, axis=0)\n",
        "\n",
        "# Binarize labels for ROC computation\n",
        "num_classes = 5\n",
        "class_names = [\"Normal\", \"DCM\", \"HCM\", \"MINF\", \"ARV\"]\n",
        "y_true_bin = label_binarize(all_labels, classes=np.arange(num_classes))\n",
        "\n",
        "# Compute ROC curve and AUC for each class\n",
        "fpr, tpr, roc_auc = {}, {}, {}\n",
        "for i in range(num_classes):\n",
        "    fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], all_probs[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "# ==========================\n",
        "# Plot ROC Curves\n",
        "# ==========================\n",
        "plt.figure(figsize=(8, 6))\n",
        "for i, label in enumerate(class_names):\n",
        "    plt.plot(fpr[i], tpr[i], lw=2, label=f\"{label} (AUC = {roc_auc[i]:.2f})\")\n",
        "\n",
        "plt.plot([0, 1], [0, 1], \"k--\", lw=1)\n",
        "plt.title(\"ROC Curve - 3D CNN (Validation Set)\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "u2XJMx2O9BO6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}